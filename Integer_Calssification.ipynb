{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Integer_Calssification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvi7SzCH1CsP",
        "colab_type": "code",
        "outputId": "f4fbc5e2-ce1e-4985-e5e9-724f77503655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import IPython\n",
        "from six.moves import urllib\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s06-6tjx1kyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading dataset and splitting. Using MNIST dataset\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
        "\n",
        "#reshape to specify they are a single channel\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KXmVz2r2N2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pre-Process data by scaling pixel values to a rangeof 0 to 1. Divide values by 255\n",
        "#Images are a 28X28 NPArray with pixel values 0-255, & labels of an array of integers\n",
        "#ranigng from 0-9\n",
        "\n",
        "#Process both training and test sets in the same way\n",
        "def preprocess_images(imgs):\n",
        "    sample_img = imgs if len(imgs.shape) == 2 else imgs[0]\n",
        "    assert sample_img.shape in [(28,28,1), (28,28)], sample_img.shape \n",
        "    #Above line: assuring images are 28x28 with single channel (grayscale)\n",
        "    \n",
        "    return imgs / 255.0\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DNTVkdp3He_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqStC_013NUI",
        "colab_type": "code",
        "outputId": "b45b1443-8814-4312-92b4-239551250bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "#Verifying data is in correct format in order to assure accurate feeding to model\n",
        "plt.figure(figsize=(10,2))\n",
        "\n",
        "for i in range(6):\n",
        "    #Display first six images\n",
        "    plt.subplot(1,6,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i].reshape(28,28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(train_labels[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAABvCAYAAADvyMT/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5LT/B1AIEbvbfDWiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x144 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSN6TV9437MN",
        "colab_type": "code",
        "outputId": "ecf9d36d-54ac-47cc-cbcf-abf859806ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "#Building Model\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "#32 convolution filters used, each of size 3x3\n",
        "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
        "#Another 62 convolution filers used, same size\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "#Choose best features thru pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#Turn neurons on and off at random. Improves convergence\n",
        "model.add(Dropout(0.25))\n",
        "#Flatten as there are too many dimension and we only want the calssification output\n",
        "model.add(Flatten())\n",
        "#Fully connect all layers to get all relevant data\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#Add in another dropout for reason explained above\n",
        "model.add(Dropout(0.5))\n",
        "#Now, output a softmax in order to condense the matrix into output probabilities\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0811 19:06:57.684547 139744925734784 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdUTam8m5fTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile model. Add Loss Function: Measures accuracy during training. Goal is to minimize.\n",
        "#Optimizer: How model is updated based on loss function accuracy\n",
        "#Metrics: Used to monitor training and testing. \"Accuracy\" is % of images correctly classified\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowPriLP67Cz",
        "colab_type": "code",
        "outputId": "c1a0c8c1-ae10-4d94-f971-105cd85b299d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "#Fit model to train and test data using an 5 epochs.\n",
        "clf = model.fit(train_images, train_labels, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 9s 155us/sample - loss: 0.1913 - acc: 0.9408\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0818 - acc: 0.9758\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0628 - acc: 0.9808\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0511 - acc: 0.9841\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0445 - acc: 0.9866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-iUkNhu7_8e",
        "colab_type": "code",
        "outputId": "2f951b6c-bca5-4743-e9f3-4acd6aa4c516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#Compare model performance to test data\n",
        "\n",
        "print(test_images.shape)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print('Test Acc.:', test_acc )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 28, 28, 1)\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: 0.0273 - acc: 0.9922\n",
            "Test Acc.: 0.9922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-pZZAU09im8",
        "colab_type": "text"
      },
      "source": [
        "Accuracy on test set can be worse than train set, due to overfitting of the training set. \n",
        "\n",
        "In this case our testing accuracy is better than the training accuracy due to reglurization from the Dropout layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF0K3YMZ-MYe",
        "colab_type": "text"
      },
      "source": [
        "#Animation showing accuracy of model based on other datasets. (Still Wokring on)\n",
        "\n",
        "Courtesy of Lex Fridman and the MIT DeepLearning Basics Repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsQ6MQ6R9XOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "this_repo_url = 'https://github.com/lexfridman/mit-deep-learning/raw/master/'\n",
        "this_tutorial_url = this_repo_url + 'tutorial_deep_learning_basics'\n",
        "\n",
        "mnist_dream_path = 'images/mnist_dream.mp4'\n",
        "mnist_prediction_path = 'images/mnist_dream_predicted.mp4'\n",
        "\n",
        "# download the video if running in Colab\n",
        "if not os.path.isfile(mnist_dream_path): \n",
        "    print('downloading the sample video...')\n",
        "    vid_url = this_tutorial_url + '/' + mnist_dream_path\n",
        "    \n",
        "    mnist_dream_path = urllib.request.urlretrieve(vid_url)[0]\n",
        "                                                                                                  \n",
        "def cv2_imshow(img):\n",
        "    ret = cv2.imencode('.png', img)[1].tobytes() \n",
        "    img_ip = IPython.display.Image(data=ret)\n",
        "    IPython.display.display(img_ip)\n",
        "\n",
        "cap = cv2.VideoCapture(mnist_dream_path) \n",
        "vw = None\n",
        "frame = -1 # counter for debugging (mostly), 0-indexed\n",
        "\n",
        "# go through all the frames and run our classifier on the high res MNIST images as they morph from number to number\n",
        "while True: # should 481 frames\n",
        "    frame += 1\n",
        "    ret, img = cap.read()\n",
        "    if not ret: break\n",
        "               \n",
        "    assert img.shape[0] == img.shape[1] # should be a square\n",
        "    if img.shape[0] != 720:\n",
        "        img = cv2.resize(img, (720, 720))\n",
        "       \n",
        "    #preprocess the image for prediction\n",
        "    img_proc = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_proc = cv2.resize(img_proc, (28, 28))\n",
        "    img_proc = preprocess_images(img_proc)\n",
        "    img_proc = 1 - img_proc # inverse since training dataset is white text with black background\n",
        "\n",
        "    net_in = np.expand_dims(img_proc, axis=0) # expand dimension to specify batch size of 1\n",
        "    net_in = np.expand_dims(net_in, axis=3) # expand dimension to specify number of channels\n",
        "    \n",
        "    preds = model.predict(net_in)[0]\n",
        "    guess = np.argmax(preds)\n",
        "    perc = np.rint(preds * 100).astype(int)\n",
        "    \n",
        "    img = 255 - img\n",
        "    pad_color = 0\n",
        "    img = np.pad(img, ((0,0), (0,1280-720), (0,0)), mode='constant', constant_values=(pad_color))  \n",
        "    \n",
        "    line_type = cv2.LINE_AA\n",
        "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.3        \n",
        "    thickness = 2\n",
        "    x, y = 740, 60\n",
        "    color = (255, 255, 255)\n",
        "    \n",
        "    text = \"Neural Network Output:\"\n",
        "    cv2.putText(img, text=text, org=(x, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
        "                    color=color, lineType=line_type)\n",
        "    \n",
        "    text = \"Input:\"\n",
        "    cv2.putText(img, text=text, org=(30, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
        "                    color=color, lineType=line_type)   \n",
        "        \n",
        "    y = 130\n",
        "    for i, p in enumerate(perc):\n",
        "        if i == guess: color = (255, 218, 158)\n",
        "        else: color = (100, 100, 100)\n",
        "            \n",
        "        rect_width = 0\n",
        "        if p > 0: rect_width = int(p * 3.3)\n",
        "        \n",
        "        rect_start = 180\n",
        "        cv2.rectangle(img, (x+rect_start, y-5), (x+rect_start+rect_width, y-20), color, -1)\n",
        "\n",
        "        text = '{}: {:>3}%'.format(i, int(p))\n",
        "        cv2.putText(img, text=text, org=(x, y), fontScale=font_scale, fontFace=font_face, thickness=thickness,\n",
        "                    color=color, lineType=line_type)\n",
        "        y += 60\n",
        "    \n",
        "    # if you don't want to save the output as a video, set this to False\n",
        "    save_video = True\n",
        "    \n",
        "    if save_video:\n",
        "        if vw is None:\n",
        "            codec = cv2.VideoWriter_fourcc(*'DIVX')\n",
        "            vid_width_height = img.shape[1], img.shape[0]\n",
        "            vw = cv2.VideoWriter(mnist_prediction_path, codec, 30, vid_width_height)\n",
        "        # 15 fps above doesn't work robustly so we right frame twice at 30 fps\n",
        "        vw.write(img)\n",
        "        vw.write(img)\n",
        "    \n",
        "    # scale down image for display\n",
        "    img_disp = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
        "    cv2_imshow(img_disp)\n",
        "    IPython.display.clear_output(wait=True)\n",
        "        \n",
        "cap.release()\n",
        "if vw is not None:\n",
        "    vw.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9RQLOxc-ctT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}